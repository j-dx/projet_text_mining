# -*- coding: utf-8 -*-
"""CODE_FINAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XQnK7328eA_6J9hF7179nvQaiD33nvCT

# Chatbot

## Importation des packages
"""

import pickle
import dill as pickle
import pandas as pd
import spacy
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
import re
import sys
import pickle
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer # ou CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
#from nltk.corpus import stopwords
from google.colab import drive
import tensorflow as tf
import pandas as pd

#!pip install -U spacy
#!python -m spacy download fr_core_news_sm
nlp = spacy.load('fr_core_news_sm')

"""## Lecture des fichiers contenant les données, les tokenisers, les modèles"""

matrice_td_faq = pickle.load(open('matrice_td_faq', 'rb'))

vectorizer_faq = pickle.load(open('vectorizer_faq', 'rb'))

model_binaire = pickle.load(open('classif_binaire_proba', 'rb'))

vector_binaire = pickle.load(open('bin_vectorizer_proba', 'rb'))

faq = pickle.load(open('faq', 'rb'))

texts = pickle.load(open('texts', 'rb'))

idx2char = pickle.load(open('idx2carCOURT', 'rb'))

char2idx = pickle.load(open('car2idxCOURT', 'rb'))

"""Fonction qui permet de lemmatiser le texte"""

def lem(text):
    """ Met à l'infinitif les verbes et met la racine des mots """
    tw_nlp = nlp(text) # spacy
    tokens = []
    for token in tw_nlp:
        tokens.append(token.lemma_)
    return ' '.join(tokens)

def substitute_url(text, url_replacement='URLEXPR'):
  """ Remplace les urls par URLEXPR """
  txt = re.sub(r'(.+?)?(http.?://.+?)\s?(.+?)?',r"\1"+url_replacement+' '+r"\3",text,flags=re.IGNORECASE)
  for i in range(4) : 
    txt = re.sub(r'(.+?)?(http.?://.+?)\s?(.+?)?',r"\1"+url_replacement+' '+r"\3",txt,flags=re.IGNORECASE)
  return txt

def substitute_agence(text, replacement='AGENCE'):
  """ Remplace les noms des agences par AGENCE """
  txt = re.sub(r'(.+?)?(tui.fr)\s?(.+?)?',r"\1"+replacement+' '+r"\3",text,flags=re.IGNORECASE)
  for i in range(4) : 
    txt = re.sub(r'(.+?)?(tui.fr)\s?(.+?)?',r"\1"+replacement+' '+r"\3",txt,flags=re.IGNORECASE)
    txt = re.sub(r'(.+?)?(tui)\s?(.+?)?',r"\1"+replacement+' '+r"\3",txt,flags=re.IGNORECASE)
    txt = re.sub(r'(.+?)?(exotismes)\s?(.+?)?',r"\1"+replacement+' '+r"\3",txt,flags=re.IGNORECASE)
  return txt

"""## Importations pour la génération de texte"""

model = tf.keras.models.load_model('model_caractCOURT.h5')
#model.summary()

"""La fonction generate_text permet de générer du texte à partir du corpus."""

def generate_text(model, start_string):
  # Evaluation step (generating text using the learned model)
  start_string = start_string + "\n"
  # Number of characters to generate
  num_generate = 150

  # Converting our start string to numbers (vectorizing)
  input_eval = [char2idx[s] for s in start_string]
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty string to store our results
  text_generated = []

  # Low temperatures results in more predictable text.
  # Higher temperatures results in more surprising text.
  # Experiment to find the best setting.
  temperature = 1.0

  # Here batch size == 1
  model.reset_states()
  i = 1
  while i < num_generate : 
      predictions = model(input_eval)
      # remove the batch dimension
      predictions = tf.squeeze(predictions, 0)

      # using a categorical distribution to predict the word returned by the model
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # We pass the predicted word as the next input to the model
      # along with the previous hidden state
      input_eval = tf.expand_dims([predicted_id], 0)
      if i == 1 and idx2char[predicted_id] in '!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n' : 
        i += 1 
      else :
        if idx2char[predicted_id] in '\n' : # !.?\t
          #text_generated.append(idx2char[predicted_id])
          i = num_generate
        else : 
          text_generated.append(idx2char[predicted_id])
          i += 1

  return (''.join(text_generated))

#generate_text(model, "Bonjour toi")

"""## Création du chatbot"""

# On stock la réponse de l'utilisateur
q = input("Bonjour, je suis là pour vous aider ! ")

while(q!='stop'):
  # traitement de la question de l'utilisateur
  q1=q
  q = pd.DataFrame([q])
  q = q[0].apply(lem).apply(substitute_url).apply(substitute_agence)
  q_bin = vector_binaire.transform(q)
  q_sim = vectorizer_faq.transform(q)
  # si le modèle prédit 1 (c-a-d que la question de l'utilisateur rentre dans la thématique métier)
  if (model_binaire.predict_proba(q_bin)[0][0]<0.85):
    query_corpus_sim = np.squeeze(cosine_similarity(matrice_td_faq, q_sim))
    idx_most_sim = np.argmax(query_corpus_sim)
    ok = False
    for cl, questions in faq.items():
      for question in questions.items():
        # si la question de l'utilisateur similaire à une question 
        if str(question[0]) == texts[idx_most_sim]: 
          # alors on affiche la réponse à cette question
          print(question[1])
          ok = True
    # sinon on affiche la réponse qui est la plus similaire
    if ok == False :
      print(texts[idx_most_sim])
  # si le classifieur binaire prédit que la question de l'utilisateur n'est pas dans la thématique métier, alors on génère du texte 
  else : 
    print(generate_text(model, start_string=q1+' ')) #(q.encode('utf-8'))))   str(q, 'utf-8')
  q = input("")

